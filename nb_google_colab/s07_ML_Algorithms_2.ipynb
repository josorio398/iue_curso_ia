{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "s07_ML_Algorithms_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssanchezgoe/iue_curso_ia/blob/main/nb_google_colab/s07_ML_Algorithms_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RiEAyMBTo2s"
      },
      "source": [
        "  <tr>\n",
        "     <th><p><img alt=\"Colaboratory logo\" height=\"120 px\" src=\"http://www.redttu.edu.co/es/wp-content/uploads/2016/01/iue.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p></th> \n",
        "     <th><h1>  Algoritmos de ML I </h1></th>\n",
        "  </tr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABQ-FiIGTt5b"
      },
      "source": [
        "<p><a name=\"contents\"></a></p>\n",
        "\n",
        "# Contenido Sesión 7\n",
        "\n",
        "- <a href=\"#redela\">1. Redes elásticas</a><br>\n",
        "- <a href=\"#met\">2. Métricas</a><br>\n",
        "- <a href=\"#regpol\">3. Regresión polinómica y no lineal</a><br>\n",
        "- <a href=\"#svm\">4. Máquinas de soporte vectorial</a><br>\n",
        "- <a href=\"#gradient_descent\">5. Descenso del Gradiente</a><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3O4ISooUlm_"
      },
      "source": [
        "<p><a name=\"redela\"></a></p>\n",
        "\n",
        "# 1. Redes elásticas\n",
        "\n",
        "[[Contenidos]](#contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNw4dr2v2tsV"
      },
      "source": [
        "Como vimos anteriormente, podemos generar penalizaciones a las regresiones lineales haciendo cambios en su metrica de error.\n",
        "\n",
        "Para la regresión Ridge se penaliza con: $\\alpha \\sum w_i^2$ (penalidad $L_2$), mientras que para Lasso se tiene que: \n",
        "$\\alpha \\sum |w_i|$ (penalidad $L_1$). Cada una de ellas tenía sus pro y sus contra. Pero es posible hacer una combinación de ambos metodos.\n",
        "\n",
        "A las regresiones que usan una combinación de ambas penalidades se les conoce como **ElasticNet** (Redes elásticas) y definimos su error como:\n",
        "$$\\sum (Y_i- \\hat Y_i)^2+\\alpha \\rho \\sum |w_i| + \\frac{\\alpha(1-\\rho)}{2}\\sum w_i^2. $$\n",
        "\n",
        "Note que cuando $ \\rho=1$ tenemos la regresión Lasso, y con $\\rho=0$ tendremos la de Ridge, por tanto en las redes elásticas $0\\leq \\rho\\leq1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU8NhFaQ-UHB"
      },
      "source": [
        "Hemos dado un paso más en la complejización del modelo ya que ahora debemos preocuparnos por el ajuste de 2 hiperparámetros para seleccionar el mejor modelo.\n",
        "\n",
        "Para usar las redes elásticas en sklearn debemos importar la función 'ElasticNet' del modulo de modelos lineales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9KmzhnX-MOs"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5gRbxEi_eZg"
      },
      "source": [
        "En la implementación de sklearn tenemos los hiperparámetros 'alpha' y 'l1_ratio' ($\\rho$ en nuestra ecuación), con ellos controlaremos el comportamiento del regresor.\n",
        "\n",
        "Tenga en cuenta que para valores de *l1_ratio* $\\leq0.01$ el algoritmo de sklearn no es estable si usamos el valor de $\\alpha$ por defecto y se hace necesario que nosotros mísmos ajustemos el valor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMTIOIlLAnZ4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df=pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "X = df['ENGINESIZE'].values.reshape(-1,1)\n",
        "y = df['CO2EMISSIONS'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U7_NsiKA-L9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn import metrics\n",
        "\n",
        "#seleccionamos los datos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=3)\n",
        "\n",
        "#entrenamos el modelo\n",
        "elastic = ElasticNet(alpha=0.01,l1_ratio=1,normalize=True)\n",
        "elastic.fit(X_train,y_train)\n",
        "y_pred = elastic.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFNqiFJxBuQg"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X_test, y_test,  color='black',label=r'datos test')\n",
        "plt.scatter(X_train, y_train,  color='red',alpha=0.4,label=r'datos entrenamiento')\n",
        "plt.plot(X_test, y_pred, color='blue',label=r'predicción')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel(r'Engine size')\n",
        "plt.ylabel(r'Price')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOD-5UQ_CCce"
      },
      "source": [
        "print('MAE: ', metrics.mean_absolute_error(y_test, y_pred))\n",
        "print('MSE: ', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print('R2: ', elastic.score(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWJtcaWsGTYB"
      },
      "source": [
        "De nuevo, los hiperparámetros debemos seleccionarlos con una busqueda para determinar una buena combinación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz4BbkjgIGru"
      },
      "source": [
        "#### Ejercicio:\n",
        "\n",
        "1. En el dataset de autos (a1): elimine los datos faltantes, convierta las variables categoricas en variables dummies (a2), y separe el dataset en datos de entrenamiento y prueba.\n",
        "\n",
        "2. Use dos ciclos 'for' para recorrer $\\alpha$ en la lista [0.01,0.1,1,10,100,1000] y l1_ratio en [0.1,0.3,0.6,0.9,0.99,0.999], entrenando un modelo ElasticNet para las combinaciones de éstos y evaluandolo con RMSE. Guarde el valor de cada RMSE en un arreglo.\n",
        "\n",
        "3. Haga un mapa de calor (a3) con los valores de $\\alpha$ y l1_ratio del punto anterior y los valores de RMSE.\n",
        "\n",
        "\n",
        "a1. 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/automobileEDA.csv'\n",
        "\n",
        "a2. `pd.get_dummies()`\n",
        "\n",
        "a3. `sns.heatmap(Matriz, xticklabels, yticklabels, annot=True)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzSujHsuUbq1"
      },
      "source": [
        "<p><a name=\"met\"></a></p>\n",
        "\n",
        "# 2. Métricas\n",
        "\n",
        "[[Contenidos]](#contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGFxtafDqn9z"
      },
      "source": [
        "En la sesión anterior estudiamos un modelo de regresión simple, por medio del cual se hace una predicción calculando una suma ponderada de las características de entrada, más una constante llamada término de sesgo (también llamado intercepto).\n",
        "\n",
        "$$y=w_0+w_1x_1+w_2x_2+...+w_nx_n$$\n",
        "\n",
        "Primero necesitamos de una medida de qué tan bien (o mal) el modelo se ajusta a los datos de entrenamiento. Esta medida de evaluación (función de costo) es el error calculado entre la recta generada $\\hat{y}$ (o el hiperplano) a los puntos reales. El entrenamiento del modelo será entonces encontrar los valores de $w_i$ que minimicen dicha función de costo. Entre las métricas más populares encontramos:\n",
        "\n",
        "* Error medio absoluto (MAE)\n",
        "\n",
        "$$MAE = \\frac{1}{m}\\sum_{i=1}^{m}|\\hat{y}_i -y_i|$$\n",
        "\n",
        "* Error cuadrático medio (MSE)\n",
        "\n",
        "$$MSE=\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\hat{y}_i -y_i\\right)^2$$\n",
        "\n",
        "* Raíz del error cuadrático medio (RMSE)\n",
        "\n",
        "$$RMSE=\\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\hat{y}({\\bf x})_i -y_i\\right)^2}$$\n",
        "\n",
        "Estas métricas las podemos obtener del módulo [metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) de sklearn. Apliquémoslas al modelo lineal simple estudiado en la sesión anterior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiX9gWqvrA4K"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "archivo = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/automobileEDA.csv'\n",
        "df = pd.read_csv(archivo)\n",
        "\n",
        "#separemos nuestros datos en características y etiquetas\n",
        "X = df['engine-size'].values.reshape(-1,1)\n",
        "y = df['price'].values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB4EG-D2rHPA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "#seleccionamos los datos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=3)\n",
        "\n",
        "#entrenamos el modelo\n",
        "linear  = LinearRegression(normalize=True)\n",
        "linear.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WeuSUl1r_un"
      },
      "source": [
        "Obtengamos ahora los datos predichos por el modelo y calculemos las métricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqUwxeKutgLX"
      },
      "source": [
        "y_pred = linear.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHk4jH7isHvT"
      },
      "source": [
        "plt.scatter(X_test, y_test,  color='black',label=r'datos test')\n",
        "plt.scatter(X_train, y_train,  color='red',alpha=0.4,label=r'datos entrenamiento')\n",
        "plt.plot(X_test, y_pred, color='blue',label=r'predicción')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel(r'Engine size')\n",
        "plt.ylabel(r'Price')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIeSDGDvrP4Q"
      },
      "source": [
        "print('MAE: ', metrics.mean_absolute_error(y_test, y_pred))\n",
        "print('MSE: ', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwqtw87QrXvY"
      },
      "source": [
        "Y calculemos el coeficiente de correlación $R^2$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4U2cI0vrYoh"
      },
      "source": [
        "print('R2: ', metrics.r2_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tpklp-3rcp-"
      },
      "source": [
        "Comprobemos además si los errores se distribuyen según una distribución normal, lo que nos da una prueba de la validez de nuestro modelo. El siguiente se conoce como un *histograma de residuos*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olyl_xEzrdhQ"
      },
      "source": [
        "sns.distplot((y_test - y_pred), bins = 50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIzKAizKuQro"
      },
      "source": [
        "Una vez que hemos entrenado el modelo de regresión lineal las predicciones se obtienen rápidamente. La complejidad computacional es lineal con respecto a la cantidad de instancias y características sobre las que desea hacer predicciones. En otras palabras, hacer predicciones con el doble de instancias (o el doble de características) tomará aproximadamente el doble de tiempo de computo. Existen diferentes formas de entrenar un modelo de regresión lineal, más adecuado para casos en los que hay una gran cantidad de características o demasiadas instancias de entrenamiento para que quepan en memoria.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSHqRFKkUhCJ"
      },
      "source": [
        "<p><a name=\"regpol\"></a></p>\n",
        "\n",
        "# 3. Regresión polinómica\n",
        "\n",
        "[[Contenidos]](#contents)\n",
        "\n",
        "**Regresión Polinómica**\n",
        "\n",
        "Hasta ahora nos hemos centrado en la creación de modelos lineales simples y multivariados en donde la relación entre la/las variable/variables predictora/predictoras y la variable blanco corrsponde a una relación lineal. \n",
        "\n",
        "No obstante, en algunas ocasiones las tendencias de los datos presentan un comportamiento curvo. En estos caso debemos buscar otro modelo para representar los datos, como es el método de **regresión polinómica**. Dentro de la regreción polinómica, tenemos varias tipos de regresiones dependiendo del grado del polinómio que usemos:\n",
        "\n",
        "* Cuadrática: Si el polinomio que usamos es grado dos.\n",
        "* Cúbica: Si el polinomio que usamos es grado tres.\n",
        "* Cuártica: Si el polinomio que usamos es grado cuatro.\n",
        "* etc\n",
        "\n",
        "Podemos llamar a todos estos casos regresiones polinómicas, ya que la relación entre la variable independiente $x$ y la variable dependiente  $y$ se modela mediante un polinomio de grado n en la variable $x$:\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y}=w_0x^0+w_1x^1+w_2x^2\\cdots w_nx^x = \\sum_{i=0}^nw_ix^i\n",
        "\\end{equation}\n",
        "\n",
        "En donde los $w$'s representan los parámetros del ajuste o la regresión \n",
        "\n",
        "**¿Cómo podemos abordar una regresión polinómica?**\n",
        "\n",
        "Existe un \"truco\" que nos permite convertir una regresión polinómica en una regresión lineal múltiple. Si definimos:\n",
        "\n",
        "* $x_1=x$\n",
        "* $x_2=x^2$\n",
        "* $x_3=x^3$\n",
        "* $\\cdot$\n",
        "* $\\cdot$\n",
        "* $\\cdot$\n",
        "* $x_n=x^n$\n",
        "\n",
        "podemos tratar el problema como una regresión lineal múltiple de la forma\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y}=w_0x_0+w_1x_1+w_2x_2\\cdots w_nx_x = \\sum_{i=0}^nw_ix_i\n",
        "\\end{equation}\n",
        "\n",
        "Por ende, la regresión polinómica se considera un caso especial de la regresión lineal múltiple, de tal forma que se pueden usar los mismos mecanismos que una regresión lineal para resolver el problema del modelado de los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xAd9QqNtQSJ"
      },
      "source": [
        "**Base de datos**\n",
        "\n",
        "A continuación, tomaremos un conjunto de datos correspondientes a la clasificaciones de consumo de combustibles específicas de los modelos de carros y las emisiones estimadas de dióxido de carbono de nuevos vehiculos ligeros para la venta al por menor en Canada. Para mayor información puede consultar el [link](http://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64). La base de datos contienen características como:\n",
        "\n",
        "- **MODELYEAR** Año del vehiculo.\n",
        "- **MAKE** Marca o fabricante\n",
        "- **MODEL** Modelo del vehiculo.\n",
        "- **VEHICLE CLASS** Clase del vehiculo\n",
        "- **ENGINE SIZE** Tamaño del motor.\n",
        "- **CYLINDERS** Número de cilindros\n",
        "- **TRANSMISSION** Tipo de transmisión\n",
        "- **FUEL CONSUMPTION in CITY(L/100 km)** Consumo en ciudad en litros por cada 100 km.\n",
        "- **FUEL CONSUMPTION in HWY (L/100 km)** Consumo en autopista en litros por cada 100 km.\n",
        "- **FUEL CONSUMPTION COMB (L/100 km)** Consumo combinado en litros por cada 100 km.\n",
        "- **CO2 EMISSIONS (g/km)** Emisión en gramos por kilómetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heooCccbtX37"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Lectura de los datos\n",
        "df = pd.read_csv(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv\")\n",
        "\n",
        "# Inspección visual del dataset\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTDC1me4tm9M"
      },
      "source": [
        "Escojamos algunas características que estén relacionadas con la emisión de dioxido de carbono. Veamos la matriz de dispersión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMjwfCImttR1"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"ticks\")\n",
        "\n",
        "sns.pairplot(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJmZaLLWwFPu"
      },
      "source": [
        "Intentemos construir un modelo, mediante un ajuste polinomial, de la emisión de dióxido de carbono en función de la característica \"FUELCONSUMPTION_COMB_MPG\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgGob2LTwETj"
      },
      "source": [
        "# Escogemos nuestra variable predictora como Engine Size:\n",
        "X=df['FUELCONSUMPTION_COMB_MPG'].values.reshape(-1,1)\n",
        "\n",
        "# Escogemos nuestra variable objetivo como las emisiones de CO2:\n",
        "y=df['CO2EMISSIONS'].values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1SihdAbwUt-"
      },
      "source": [
        "**Train and test dataframe**\n",
        "\n",
        "Creemos ahora los grupos de train y test con los cuales entrenaremos y probaremos el modelo, respectivamente. Recordemos que este paso podemos hacerlo mediante la función `train_test_split` de la librería `sklearn.model_selection`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58yaYObIwUJF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=20)\n",
        "\n",
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DguZO9h6wbNK"
      },
      "source": [
        "<h2 id=\"evaluation\">Regresión polinómica de los datos:</h2>\n",
        "\n",
        "Intentemos crear un modelo cuadrático de la emisión de dióxido de carbono en función del tamaño del motor:\n",
        "\n",
        "$\\hat{y} = w_o + w_1 x + w_2 x^2$\n",
        "\n",
        "En donde x reprenta la variable independiente 'FUELCONSUMPTION_COMB_MPG'. Para resolver este problema usamos la siguiente función:\n",
        "\n",
        "\n",
        "__PloynomialFeatures()__ es una función de la  librería Scikit-learn, la cual emplea nuevo conjunto de características del conjunto de características original. Es decir, se generará una matriz compuesta de todas las combinaciones polinomiales de las características de grado menor o igual especificado en `degree`. Por ejemplo, suponiendo que el conjunto inicial solo tienen una característica, _FUELCONSUMPTION_COMB_MPG_, entonces, si especificamos `degree=2`, se generarán tres caracteristicas dadas por `degree=0`, `degree=1` y `degree=2`: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBbjcIDCwan0"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "train_x_poly = poly.fit_transform(x_train)\n",
        "train_x_poly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDbowX5owpY2"
      },
      "source": [
        "El método **fit_transform** tomo los valores de x y crea una lista, elevando los valores en potencias enteras desde 0 hasta 2.\n",
        "\n",
        "$\n",
        "\\begin{bmatrix}\n",
        "    x_1\\\\\n",
        "    x_2\\\\\n",
        "    \\vdots\\\\\n",
        "    x_n\n",
        "\\end{bmatrix}\n",
        "$\n",
        "$\\longrightarrow$\n",
        "$\n",
        "\\begin{bmatrix}\n",
        "    [ 1 & x_1 & x_1^2]\\\\\n",
        "    [ 1 & x_2 & x_2^2]\\\\\n",
        "    \\vdots & \\vdots & \\vdots\\\\\n",
        "    [ 1 & x_n & x_n^2]\n",
        "\\end{bmatrix}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJvjy6BEwu-y"
      },
      "source": [
        "De lo anterior se puede notar que se tiene la apariencia de un análisis de regresión multiple, lo que corrobora que la regresión polinómica representa un caso especial de la regresión lineal.\n",
        "\n",
        "A partir de este punto, podemos tratar nuestro problema como un caso de regresión lineal, de tal forma que podemos usar los mismos mecanismos que se emplean para resolver este tipo de problemas. Podemos usar, por ejemplo, la función  __LinearRegression()__ para solucionar nuestro problema:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxbYz2MUwuO7"
      },
      "source": [
        "clf = LinearRegression()\n",
        "train_y_ = clf.fit(train_x_poly, y_train)\n",
        "# The coefficients\n",
        "print ('Coefficients: ', clf.coef_)\n",
        "print ('Intercept: ',clf.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1oGw1D2w-D8"
      },
      "source": [
        "En la celda anterior, __Coefficient__ e __Intercept__ corresponden a los parámetros del ajuste de la linea curva. \n",
        "\n",
        "Teniendo en cuenta que estamos tratando con una regresión lineal múltiple con 3 parámetros, los cuales representan la intersección y los coeficientes del hiperplano, sklearn los calcula a partir del nuevo conjunto de características. Veamos como luce el ajuste realizado sobre los datos de \"COEMISSION\" vs \"FUELCONSUMPTION_COMB_MPG\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjdpHpYDw9Wx"
      },
      "source": [
        "plt.scatter(X, y,  color='blue')\n",
        "XX = np.arange(0.0, 60.0, 0.1)\n",
        "yy = clf.intercept_[0]+ clf.coef_[0][1]*XX+ clf.coef_[0][2]*XX*XX\n",
        "plt.plot(XX, yy, '-r')\n",
        "plt.xlabel(\"FUELCONSUMPTION_COMB_MPG\")\n",
        "plt.ylabel(\"Emission\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSw4ssPdxMVc"
      },
      "source": [
        "<h2 id=\"evaluation\">Evaluación</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-wX8_yvvggD"
      },
      "source": [
        "#con la regresion lineal\n",
        "linear = LinearRegression()\n",
        "linear.fit(x_train,y_train)\n",
        "\n",
        "y_pred = linear.predict(x_test)\n",
        "\n",
        "print('MAE: %.3f '% metrics.mean_absolute_error(y_test, y_pred))\n",
        "print('MSE: %.3f'% metrics.mean_squared_error(y_test, y_pred))\n",
        "print('RMSE: %.3f'% np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print('R2: %.3f'% metrics.r2_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdruUC3ZxLlU"
      },
      "source": [
        "#con la regresion polinómica\n",
        "from sklearn import metrics\n",
        "test_x_poly = poly.fit_transform(x_test)\n",
        "test_y_ = clf.predict(test_x_poly)\n",
        "\n",
        "print('MAE: %.3f' % metrics.mean_absolute_error(test_y_, y_test))\n",
        "print('MSE: %.3f' % metrics.mean_squared_error(test_y_, y_test))\n",
        "print('RMSE: %.3f' % np.sqrt(metrics.mean_squared_error(test_y_, y_test)))\n",
        "print('R2: %.3f'% metrics.r2_score(test_y_, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YGQeoYbxSc3"
      },
      "source": [
        "# Problema:\n",
        "\n",
        "Relice un ajuste a un polinomio de grado tres en donde aplique cada uno de los pasos mostrados la regresión polinómica anterior, y evalue si obtiene un mejor modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu6THzedvpW4"
      },
      "source": [
        "# Modelos no lineales\n",
        "\n",
        "Como bien sabemos, si los datos no presentan una tendencia lineal entre las características y las variables objetivo, debemos buscar ajustes a funciones no lineales para la construcción de modelos. Veamos algunas funciones de uso comun para el ajuste de modelos y un ejemplo práctico del crecimiento del producto interno bruto chino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-dZEGHJvu1i"
      },
      "source": [
        "Las regresiones no lineales representan una relación entre variabres independientes $x$'s y una variable dependiente $y$, lo que resulta en un modelado mediante una función no lieal de los datos. En principio, cualquier relación que no es lineal, puede representarce mediante un polinomio de grado $k$. Por ejemplo: \n",
        "\n",
        "$$ \\ y = a x^3 + b x^2 + c x + d \\ $$\n",
        "\n",
        "Además, las funciones no lineales pueden tener elementos exponenciales, logarítmicos, fracciones, entre otros. Por \n",
        "ejemplo, una función de la forma:\n",
        "\n",
        "$$ y = \\log(a x^5 + b x^3 + c x + d)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aHR4KbWvzsU"
      },
      "source": [
        "## Función exponencial\n",
        "\n",
        "Una función exponencial de base c, se define como \n",
        "\n",
        "$Y = a + b c^X$\n",
        "\n",
        "en donde $b\\neq0$, $c > 0$ , $c\\neq1$, y la X es un número real. La base, $c$, es una constante y el exponente, $X$, es una variable. Un ejemplo gráfico de la función se presenta a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvVP-IqBv268"
      },
      "source": [
        "X = np.arange(-5.0, 5.0, 0.1)\n",
        "\n",
        "Y= np.exp(X)\n",
        "\n",
        "plt.plot(X,Y) \n",
        "plt.ylabel('Variable dependiente')\n",
        "plt.xlabel('Variable independiente')\n",
        "plt.title('Función exponencial')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYDjYnYqwNNn"
      },
      "source": [
        "## Función logarítmica\n",
        "\n",
        "La función logarítmica es la función inversa de la función exponecial, y se representa como:\n",
        "\n",
        "\\begin{equation}\n",
        "y = \\log(X)\n",
        "\\end{equation}\n",
        "\n",
        "Una representación gráfica de esta función se muestra a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBw_a1GwwJug"
      },
      "source": [
        "X = np.arange(-5.0, 5.0, 0.1)\n",
        "\n",
        "Y = np.log(X)\n",
        "\n",
        "plt.plot(X,Y) \n",
        "plt.ylabel('Dependent Variable')\n",
        "plt.xlabel('Indepdendent Variable')\n",
        "plt.title('Función Logarítmica')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4qJeia9wWYh"
      },
      "source": [
        "## Función sigmoide/logística\n",
        "\n",
        "La función sigmoide tiene la forma \n",
        "\n",
        "$$ Y = a + \\frac{b}{1+ c^{(X-d)}}$$\n",
        "\n",
        "Y veremos, más adelante, en este curso, una aplicación en la la regresión logística. Una representación gráfica de la función se presenta a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAiNgQCIwXwp"
      },
      "source": [
        "X = np.arange(-5.0, 5.0, 0.1)\n",
        "\n",
        "\n",
        "Y = 1-4/(1+np.power(3, X-2))\n",
        "\n",
        "plt.plot(X,Y) \n",
        "plt.ylabel('Dependent Variable')\n",
        "plt.xlabel('Indepdendent Variable')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO5lMt14wdJj"
      },
      "source": [
        "## Ejemplo de regresión no lineal:\n",
        "\n",
        "Intentemos encontra un modelo no lineal para representar los datos del producto interno bruto de China, entre los años 1960 a 2014. El dataset se presenta a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppICr_iOwhZb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/china_gdp.csv\")\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD-rwyyzwmZa"
      },
      "source": [
        "### Gráfica del dataset ###\n",
        "\n",
        "Veamos como luce el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Evp0GDPwlMr"
      },
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "x_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\n",
        "plt.plot(x_data, y_data, 'ro')\n",
        "plt.ylabel('GDP')\n",
        "plt.xlabel('Year')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoiBpCgHwyL1"
      },
      "source": [
        "Esta gráfica parece seguir un comportamiente logístico o exponencial. El crecimiento lento del PIB empieza alrededor del año 1995, y a partir del año 2005 empieza a ser significativo, para luego caer muy poco alrededor del 2010. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8KFwFBcw8fK"
      },
      "source": [
        "### Determinación del modelo ###\n",
        "\n",
        "De una inspección inical, podemos determinar que una aproximación logística podría ser adecuada, ya que empieza a crecer lentamente, y aumenta a mitad de camino, para desacelerarse un poco al final. Veamos la siguiente función:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9e7zpDHw7W9"
      },
      "source": [
        "X = np.arange(-5.0, 5.0, 0.1)\n",
        "Y = 1.0 / (1.0 + np.exp(-X))\n",
        "\n",
        "plt.plot(X,Y) \n",
        "plt.ylabel('Dependent Variable')\n",
        "plt.xlabel('Indepdendent Variable')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wShg1xgZxB9S"
      },
      "source": [
        "De la ecuación de la función logística:\n",
        "\n",
        "$$ \\hat{Y} = \\frac1{1+e^{\\beta_1(X-\\beta_2)}}$$\n",
        "\n",
        "Tenemos que\n",
        "\n",
        "$\\beta_1$: contrala la inclinación de la curva,\n",
        "\n",
        "$\\beta_2$: proyecta la curva en x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFYiPsW1xFnl"
      },
      "source": [
        "### Construcción del modelo###\n",
        "\n",
        "Construyamos nuestro modelo de regresión e inicialicemos los parámetros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsPvAo5LxJAL"
      },
      "source": [
        "def sigmoid(x, Beta_1, Beta_2):\n",
        "     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n",
        "     return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiphTYtLxQYj"
      },
      "source": [
        "Veamos un ajuste preliminar \"a  mano\" de una función sigmoide a los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCYrTXNSxVxV"
      },
      "source": [
        "beta_1 = 0.10\n",
        "beta_2 = 2010.0\n",
        "\n",
        "#logistica\n",
        "Y_pred = sigmoid(x_data, beta_1 , beta_2)\n",
        "\n",
        "#plot predicción inicial\n",
        "plt.plot(x_data, Y_pred*15000000000000.)\n",
        "plt.plot(x_data, y_data, 'ro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2316O339xamZ"
      },
      "source": [
        "El objetivo en este modelo es encontrar los parámetros que mejor ajustan la curva a los datos. Procedamos a normalizarlos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUZqDr_4xejH"
      },
      "source": [
        "# normalicemos los datos\n",
        "xdata =x_data/max(x_data)\n",
        "ydata =y_data/max(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qah9E6YRxiPh"
      },
      "source": [
        "Usemos el método __curve_fit__ `scipy`, que usa un ajuste de mínimos cuadrados no lineal para ajustar nuesta función sigmoide a los datos. El algoritmo ajusta iterativamente los parámetros, de tal forma que la suma de los residuos cuadrados $sig(x_{data}, *popt) - y_{data}$ se minimiza."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71QWbpS2xnhb"
      },
      "source": [
        "from scipy.optimize import curve_fit\n",
        "popt, pcov = curve_fit(sigmoid, xdata, ydata)\n",
        "#imprimamos los parámetros\n",
        "print(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kum7lNGixrN4"
      },
      "source": [
        "# Visualización del modelo\n",
        "x = np.linspace(1960, 2015, 55)\n",
        "x = x/max(x)\n",
        "plt.figure(figsize=(8,5))\n",
        "y = sigmoid(x, *popt)\n",
        "plt.plot(xdata, ydata, 'ro', label='data')\n",
        "plt.plot(x,y, linewidth=3.0, label='fit')\n",
        "plt.legend(loc='best')\n",
        "plt.ylabel('GDP')\n",
        "plt.xlabel('Year')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKYbwe7QxvsD"
      },
      "source": [
        "## Problema:\n",
        "Evalue la precisión del modelo creado anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5u7hEdbUpt8"
      },
      "source": [
        "<p><a name=\"svm\"></a></p>\n",
        "\n",
        "# 4. Máquinas de Soporte Vectorial (Support Vector Machine SVM)\n",
        "\n",
        "[[Contenidos]](#contents)\n",
        "\n",
        "SVM son algoritmos de Machine Learning desarrollados por  Vladimir Vapnik y su equipo en los laboratorios AT&T. Estos algoritmnos sirven tanto para solucionar problemas de clasificación como de regresión. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QLIss6OUOgZ"
      },
      "source": [
        "## Intuición inicial\n",
        "\n",
        "Supogamos que tenemos un dataset con dos características y  queremos clasificar con una línea recta (hiperplano en el caso de mas dimensiones) a que grupo (rojos o azules) corresponde cada uno. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZJE1tMKXGnb"
      },
      "source": [
        "<p><img height=\"230px\" src=\"https://miro.medium.com/max/300/0*9jEWNXTAao7phK-5.png\"  hspace=\"10px\" vspace=\"0px\">\n",
        "  <img height=\"230px\" src=\"https://miro.medium.com/max/300/0*0o8xIA4k3gXUDCFU.png\"  hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0V7RqCYHXBY"
      },
      "source": [
        "En la figura de la izquierda, podemos ver que tenemos \"infinitas\" rectas que sirven para clasificar los dos grupos. En SVM escogemos la recta (hiperplano) que maximiza el margen, entendiendo como el margen la distacia maxima que hay entre los puntos de las dos clases. Los puntos que están mas cerca de la frontera (línea punteada) se conocen como los vectores de soporte, estos pueden quedar sobre la línea de frontera o fuera de ella. En la figura de la derecha son datos que aparecen con el color relleno.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S7zfRPZYj2C"
      },
      "source": [
        "## Regresión\n",
        "\n",
        "Los regresores basado en máquinas de soporte vectorial se suele denota como SVR (Support Vector Regressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-tiwHj6YbmF"
      },
      "source": [
        "### SVR Lineal\n",
        "\n",
        "En este caso, la idea es seleccionar el hiperplano regresor que mejor se ajuste a nuestro conjunto de datos de entrenamiento. Ahora no disponemos de clases\n",
        "para separar. La idea se basa en considerar una distancia margen ε, de modo que esperamos que todas las instancias se encuentren en una banda o tubo entorno a nuestro hiperplano, es decir, que disten una cantidad menor de ε del hiperplano. A\n",
        "la hora de definir el hiperplano sólo se consideran las instancias que disten más de ε de nuestro hiperplano. En este caso esas instancias serán los considerados como\n",
        "vectores soporte.\n",
        "\n",
        "\n",
        "<p><img height=\"300px\" src=\"https://www.saedsayad.com/images/SVR_2.png\"  align=\"center\" vspace=\"0px\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqFkQcNLZMtT"
      },
      "source": [
        "Por medio de la variable $C$ podemos controlar la penalización de las clasificaciones erróneas:\n",
        "\n",
        "1. Un valor grande de $C$ corresponde a penalizaciones grandes en el error relacionado con las malas clasificaciones. \n",
        "\n",
        "2. Valores pequeños de $C$ corresponden a una penalización de los errores menor, respecto a los a las malas clasificaciones.\n",
        "\n",
        "Mediante el hiperparámetro $C$ podemos controlar el ancho del margen y de esta forma ajustar el compromiso sesgo-varianza.\n",
        "\n",
        "El concepto anterior se realaciona con la regularizacion vista en un ajuste lineal en donde una disminución de $C$ aumenta el sesgo y disminuye la varianza."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFvVJTztzHTK"
      },
      "source": [
        "import sklearn.svm as svm  \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNMojoMMcYQW"
      },
      "source": [
        "df=pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T94lwCBUfGzk"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blAxyGEBzWDV"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBzn8fj7eDkn"
      },
      "source": [
        "Hagamos un primer ajuste lineal. En la matriz de dispersión vimos que existe una relación lineal entre la emisión de CO2 y el tamaño del motor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62FDzLNdfapU"
      },
      "source": [
        "df.plot(x='ENGINESIZE', y='CO2EMISSIONS',kind='scatter')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8HvcembhKWT"
      },
      "source": [
        "X = df['ENGINESIZE'].values.reshape(-1,1)\n",
        "y = df['CO2EMISSIONS'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
        "\n",
        "estimator=svm.LinearSVR()\n",
        "estimator.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt6R-_1BlS5h"
      },
      "source": [
        "print('R2-train: ', estimator.score(X_train, y_train))\n",
        "print('R2-test: ', estimator.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQq-Cd6UlSrh"
      },
      "source": [
        "y_pred = estimator.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqSCfJZlSb5"
      },
      "source": [
        "plt.scatter(X_test,y_test, label='Datos')\n",
        "plt.plot(X_test,y_pred, color='r', label='Predicción')\n",
        "plt.xlabel('ENGINESIZE')\n",
        "plt.ylabel('CO2EMISSIONS')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8jGOkKbmoX9"
      },
      "source": [
        "Con SVR se pueden hacer regresiones multilineales. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t6N4_TylSK9"
      },
      "source": [
        "X = df[['ENGINESIZE','FUELCONSUMPTION_CITY','CYLINDERS']]\n",
        "y = df['CO2EMISSIONS'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
        "\n",
        "estimator=svm.LinearSVR()\n",
        "estimator.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b52MvQDknP03"
      },
      "source": [
        "print('R2_train: ', estimator.score(X_train, y_train))\n",
        "print('R2_test: ', estimator.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwsGmYv3pz1e"
      },
      "source": [
        "y_pred=estimator.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRjCWDXJvGER"
      },
      "source": [
        "Cuando el número de caracteríticas es mayor a dos y no se puede visualizar el resultado de la predicción, podemos graficar los resultados de los datos de prueba con los obtenidos a partir de la predicción. La interpretación de esta gráfica se puede entender como que una buena predicción se da para aquellos que se encuentren cerca de un línea con pendiente 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb5qpvkBqprI"
      },
      "source": [
        "plt.scatter(y_test,y_pred,color='orange')\n",
        "\n",
        "plt.plot(np.arange(100,600,100),np.arange(100,600,100), 'k')\n",
        "plt.xlabel('y_prueba')\n",
        "plt.ylabel('y_prediccion')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP4YjySpaDWQ"
      },
      "source": [
        "### SVR con kernel\n",
        "\n",
        "Para el caso en el que la función que se desea ajustar no puede ser ajustada con una recta, el método que se utiliza es mapear los puntos a una mayor dimensionalidad en la que si se pueda hacer el ajuste lineal y luego la solución dada se mapea de regreso al espacio original. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr6XYNbFaPCt"
      },
      "source": [
        "\n",
        "<p><img height=\"300px\" src=\"https://www.saedsayad.com/images/SVR_5.png\"  align=\"center\" vspace=\"0px\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORqS54RZrCOF"
      },
      "source": [
        "La posibles funciones de Kernel integrados en scikitlearn son: 'linear', 'poly', 'rbf', 'sigmoid'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z56jeKiC70zN"
      },
      "source": [
        "Analicemos el caso de dos variables que es posible ver que no tienen una dependencia lineal, como FUELCONSUMPTION_COMB_MPG, FUELCONSUMPTION_CITY. Observe los resultados que se obtienen usando diferentes kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZaNpO1I7j9P"
      },
      "source": [
        "plt.scatter(df['FUELCONSUMPTION_COMB_MPG'], df['FUELCONSUMPTION_CITY'])\n",
        "plt.xlabel('FUELCONSUMPTION_COMB_MPG')\n",
        "plt.ylabel('FUELCONSUMPTION_CITY')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUwlKIuK8XTS"
      },
      "source": [
        "### Kernel lineal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QozBVET68p4G"
      },
      "source": [
        "X = df['FUELCONSUMPTION_COMB_MPG'].values.reshape(-1,1)\n",
        "y = df['FUELCONSUMPTION_CITY'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
        "\n",
        "estimator=svm.SVR(kernel='linear', gamma='auto')\n",
        "estimator.fit(X_train, y_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I92KQoGI8p4c"
      },
      "source": [
        "y_pred = estimator.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5TasHp68p4f"
      },
      "source": [
        "plt.scatter(X_test,y_test, label='Datos')\n",
        "plt.scatter(X_test,y_pred, color='r', label='Predicción')\n",
        "plt.xlabel('FUELCONSUMPTION_COMB_MPG')\n",
        "plt.ylabel('FUELCONSUMPTION_CITY')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l0w-hfQewui"
      },
      "source": [
        "print('R2_train: ', estimator.score(X_train, y_train))\n",
        "print('R2_test:', estimator.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzK9fx5k8iY4"
      },
      "source": [
        "### Kernel rbf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8fmrPAOra7m"
      },
      "source": [
        "X = df['FUELCONSUMPTION_COMB_MPG'].values.reshape(-1,1)\n",
        "y = df['FUELCONSUMPTION_CITY'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n",
        "\n",
        "estimator=svm.SVR(kernel='rbf', gamma='auto')\n",
        "estimator.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsOh-TIkwork"
      },
      "source": [
        "y_pred = estimator.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0FgJ7wUwsow"
      },
      "source": [
        "plt.scatter(X_test,y_test, label='Datos')\n",
        "plt.scatter(X_test,y_pred, color='r', label='Predicción')\n",
        "plt.xlabel('FUELCONSUMPTION_COMB_MPG')\n",
        "plt.ylabel('FUELCONSUMPTION_CITY')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tRwIyqO81Dt"
      },
      "source": [
        "print('R2_train:', estimator.score(X_train, y_train))\n",
        "print('R2_test:', estimator.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd1LWkUae_ZB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AriIcvo55MZZ"
      },
      "source": [
        "<p><a name=\"gradient_descent\"></a></p>\n",
        "\n",
        "#5. Gradiente descendente\n",
        "\n",
        "Existen formas diferentes de entrenar un modelo de regresión lineal, más adecuado para casos en los que, como mencionamos, hay una gran cantidad de características o demasiadas instancias de entrenamiento para que quepan en la memoria. Un algoritmo con estas características es el del gradiente descendente estocástico (SGD).\n",
        "\n",
        "La idea general del algoritmo de gradiente descendente es ajustar los parámetros $w_i$ de forma iterativa para minimizar la función de costo. Básicamente, este algoritmo mide el gradiente local de la función de costo con respecto a los parámetros $w_i$, y va en la dirección del gradiente descendente. Una vez que el gradiente es cero se ha alcanzado un mínimo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2QKpTgU5Hle"
      },
      "source": [
        "<img height=\"300px\" src=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0402.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S30KToy5So8"
      },
      "source": [
        "Un parámetro importante es el tamaño de los pasos, determinado por el hiperparámetro $\\eta$, conocido como *tasa de aprendizaje*. Si este es muy pequeño, el algoritmo tomará mucho tiempo en converger al mínimo. Por el contrario si este es muy grande, se corre el peligro de alejarse mucho más del mínimo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC0ICV7h4Hrx"
      },
      "source": [
        "<img height=\"240px\" src=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0407.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR9vt6vB5WbU"
      },
      "source": [
        "Concretamente, el SGD selecciona una instancia aleatoria en el conjunto de entrenamiento en cada paso y calcula los gradientes basado solo en esa instancia. Esto es lo que hace que el algoritmo sea mucho más rápido, ya que tiene muy pocos datos para manipular en cada iteración. \n",
        "\n",
        "La clase `LinearRegression` de sklearn que hemos utilizado no utiliza este algoritmo. Para implementar una regresión lineal usando SGD con Scikit-Learn, se puede usar la clase `SGDRegressor`, que por defecto optimiza la función de costo MSE. Vamos a generar algunos datos de aspecto lineal para probar probar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw6qmDe-5aAw"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbv30WQP-6za"
      },
      "source": [
        "w0, w1 = sgd_reg.intercept_, sgd_reg.coef_\n",
        "\n",
        "print(w0,w1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48n5RY3_-_bk"
      },
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X, [w0 + w1 * x for x in X], \"r-\", label=\"Predicciones SGD\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}